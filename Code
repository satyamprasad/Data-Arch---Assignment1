generate synthetic data using faker library for order management for supply chain management use case which is designed for a company which manufactures laptops, servers, peripherals. this data should have the order id quantity make and model, brand, Quantity, Unit Price, Total Price etc
https://gist.githubusercontent.com/provpup/2fc41686eab7400b796b/raw/b575bd01a58494dfddc1d6429ef0167e709abf9b/hamlet.txt
/***23 June****/


Making a list parallelized
rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])

#To Show all the elements 
rdd.collect() 

# First 3 elements
rdd.take(3)          

# Number of elements
rdd.count()         

# First element
rdd.first()          

#Flat Map
rdd_flatmap = rdd.flatMap(lambda x: (x, x*10))
rdd_flatmap.collect()

# Reduce to a single value by adding each element of the list 
rdd.reduce(lambda x, y: x + y)  

#Multiplying every element by 2 or any number and show the result 
rdd_map = rdd.map(lambda x: x * 2)
rdd_map.collect()

#Filter list value which is even number
rdd_filter = rdd.filter(lambda x: x % 2 == 0)
rdd_filter.collect()

Create a Data Frame:

#Creating Data Frame using range function 

df_range = spark.range(10)  
df_range.show()

#Creating Data Frame using your dataset and defining the Data Frame and show the result 

data = [("Mark", 35), ("Bob", 25), ("Charlie", 38)]
df = spark.createDataFrame(data, ["Name", "Age"])
df.show()

df_selected = df.select("Name")
df_selected.show()

df_multi_select = df.select("Name", "Age")

df_multi_select.show()


from pyspark.sql.functions import col (if you get issue)

# Now your filter command will work 
df_filtered = df.filter(col("Age") > 1)
df_filtered.show()

# Using SQL-like syntax
df_filtered_sql = df.filter("Age > 1")
df_filtered_sql.show()

#Define a alias
df_multi_select = df.select(col("Name"), col("Age").alias("YearsOld"))
df_multi_select.show()

Add a new column:
from pyspark.sql.functions import lit 

df_with_col = df.withColumn("City", lit("New York"))
df_with_col.show()

Doing calculation on column:

df_calc_col = df.withColumn("AgeInMonths", col("Age") * 12)
df_calc_col.show()

Drop a column:
df_no_age = df.drop("Age")
df_no_age.show()

Group by and aggregate:

# Assuming df_sales has 'ProductType' and 'Revenue'
# For our simple df, let's pretend 'Name' is product type and 'Age' is quantity

df_agg = df.groupBy("Name").agg({"Age": "sum"})
df_agg.show()

from pyspark.sql.functions import sum, avg
df_agg_multi = df.groupBy("Name").agg(sum("Age").alias("TotalQuantity"), avg("Age").alias("AvgQuantity"))
df_agg_multi.show()

Sort (Order By):
df_sorted = df.orderBy("Age", ascending=False)
df_sorted.show()

#Reading a CSV file (assuming path and file is accessible from Spark):
df_csv = spark.read.csv("path/to/your/file.csv", header=True, inferSchema=True)
df_csv.show(5)

df_csv.show() # Shows first 20 rows by default
df_csv.show(5, truncate=False) # Show first 5 rows, don't truncate long strings

Count rows:
row_count = df_csv.count()
print(f"Number of rows: {row_count}")

Collect data (bring to driver as Python list):
all_data = df_csv.collect()
for row in all_data:
    print(row)

(*Use collect() with caution on very large DataFrames, as it brings all data to the driver's memory.)
Take a sample of rows:

sample_rows = df_csv.take(2)
print(sample_rows)
Write to file:
df.write.csv("my_data_output.csv", header=True, mode="overwrite")
df.write.parquet("my_data_output.parquet", mode="overwrite")

(mode="overwrite" will overwrite if the directory already exists. These files will be created in very small pieces. How can we make one file?)
It will create a folder with many files 


#Reading a Parquet file:
df_parquet = spark.read.parquet("path/to/your/data.parquet")
df_parquet.show(5)


When you read this file it will give the proper output so breaking the files will be internal to spark.
Print schema:

df_parquet.printSchema()
Describe (summary statistics):
df.describe().show()
# Assuming df_csv is your DataFrame
df_csv.describe().show(truncate=False)

from pyspark.sql.functions import col

# Only describe numeric columns
df_csv.select("Quantity", "Unit Price", "Total Price").describe().show()


pandas_df_description = df_csv.describe().toPandas()

print(pandas_df_description)

Use toPandas() with extreme caution on very large Spark DataFrames, as it pulls ALL data into your driver's memory. However, for the output of describe(), which is usually small (only a few rows), it's generally safe.



/************************** Spark SQL **************************/


Spark SQL is Apache Sparkâ€™s module for working with structured data. The SQL Syntax section describes the SQL syntax in detail along with usage examples when applicable. This document provides a list of Data Definition and Data Manipulation Statements, as well as Data Retrieval and Auxiliary Statements.
DDL Statements
Data Definition Statements are used to create or modify the structure of database objects in a database. Spark SQL supports the following Data Definition Statements:

CREATE DATABASE
CREATE FUNCTION
CREATE TABLE
CREATE VIEW
ALTER DATABASE
ALTER TABLE
ALTER VIEW
DECLARE VARIABLE
DROP DATABASE
DROP FUNCTION
DROP TABLE
DROP TEMPORARY VARIABLE
DROP VIEW
REPAIR TABLE
TRUNCATE TABLE
USE DATABASE
DML Statements

Data Manipulation Statements are used to add, change, or delete data. Spark SQL supports the following Data Manipulation Statements:
INSERT TABLE
INSERT OVERWRITE DIRECTORY
LOAD

Data Retrieval StatementsSpark supports 

SELECT statement that is used to retrieve rows from one or more tables according to the specified clauses. The full syntax and brief description of supported clauses are explained in the SELECT section. The SQL statements related to SELECT are also included in this section. Spark also provides the ability to generate logical and physical plan for a given query using EXPLAIN statement.

SELECT Statement

SHOW COLUMNS
SHOW CREATE TABLE
SHOW DATABASES
SHOW FUNCTIONS
SHOW PARTITIONS
SHOW TABLE EXTENDED
SHOW TABLES
SHOW TBLPROPERTIES
SHOW VIEWS
UNCACHE TABLE

*******************************************

Read a text file and count the words it has and print the count on console
**************************************************************************

from pyspark import SparkContext

# Step 1: Initialize Spark Context
sc = SparkContext(appName="WordFrequencyCountRDD")

# Step 2: Load the text file
rdd = sc.textFile("/path/to/your/textfile.txt")  #  Replace with actual file path

# Step 3: Split lines into words
words = rdd.flatMap(lambda line: line.lower().split())

# Step 4: Map each word to a (word, 1) tuple
word_pairs = words.map(lambda word: (word, 1))

# Step 5: Reduce by key (sum up all 1s per word
word_counts = word_pairs.reduceByKey(lambda a, b: a + b)

# Step 6: Collect and print the result
for word, count in word_counts.collect():
    print(f"{word}: {count}")


# Step 7: Stop Spark context
sc.stop()

from pyspark.sql import SparkSession, Row

# Step 1: Start Spark session
spark = SparkSession.builder.appName("WordCountWithStopwords").getOrCreate()
sc = spark.sparkContext

# Step 2: Define stopwords (you can expand this list)
stopwords = {"to", "if", "the", "in", "for", "and", "on", "be", "a", "an", "of", "with", "at", "by", "is", "it", "this"}

# Step 3: Read input text file
rdd = sc.textFile("/Users/alokranjan/input.txt")

# Step 4: Tokenize and clean
words = rdd.flatMap(lambda line: line.lower().split()) \
           .filter(lambda word: word.isalpha() and word not in stopwords)

# Step 5: Word count
word_counts = words.map(lambda word: (word, 1)) \
                   .reduceByKey(lambda a, b: a + b)

# Step 6: Convert to DataFrame
word_df = word_counts.map(lambda x: Row(word=x[0], count=x[1])).toDF()

# Optional: Show result
word_df.orderBy("count", ascending=False).show(20)

# Optional: Save to file
word_df.write.mode("overwrite").option("header", True).csv("/Users/alokranjan/cleaned_word_count_csv")

# Step 7: Stop Spark session
spark.stop()

*****************************************************************
Plot a histogram 
*****************************************************************
import matplotlib.pyplot as plt

# Convert Spark DataFrame to Pandas
pandas_df = word_df.toPandas()

# Sort by count (optional)
pandas_df = pandas_df.sort_values(by="count", ascending=False)

# Plot histogram
plt.figure(figsize=(10, 6))
plt.bar(pandas_df["word"], pandas_df["count"], color='skyblue')
plt.xlabel("Words")	
plt.ylabel("Frequency")
plt.title("Word Frequency Histogram")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

##############################2/7########################################
/*************************. Install MongoDB on Ubuntu ***************************

  cat /etc/lsb-release
  sudo apt-get install gnupg curl (Not required)
 	
  curl -fsSL https://www.mongodb.org/static/pgp/server-8.0.asc |sudo gpg -o /usr/share/keyrings/mongodb-server-8.0.gpg --dearmor
 
   echo "deb [ arch=amd64,arm64 signed-by=/usr/share/keyrings/mongodb-server-8.0.gpg ] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/8.0 multiverse" | sudo tee /etc/apt/sources.list.d/mongodb-org-8.0.list
   
   sudo apt-get update
   
   sudo apt-get install -y mongodb-org

sudo systemctl start mongod

sudo systemctl daemon-reload

sudo systemctl status mongod

sudo systemctl enable mongod

sudo systemctl restart mongod

mongosh

help
db.help()
db.tablename.help
db. (tab tab)
dp.tablename. (tab tab)


db.createCollection("incidents");

db.incidents.insert({_id:1, name:"Server Issue"});
db.incidents.insert({_id:2, name:"DB Issue"});
db.incidents.insert({_id:3, name:"Application Issue"});


db.createCollection("databases");

db.incidents.insert({name:"XYZ Issue"});
db.incidents.insert({name:"ABCD Issue"});
db.incidents.insert({name:"MNOP Issue"});


show tables;

#Json Object

Student Collection (Table) 

{
name: "Srinivas",
email: "sriniwas@gmail.com",
mob:9999999999,
salary: 111111,
depart:"IT Support"
}


db.student.insert({
name: "Sivas",
email: "sriniwas@gmail.com",
mob:9999999999,
salary: 111111,
depart:"IT Support"
});


;


db.student.insert({
name: "Srinivas",
email:
{
personal_email: "xyz@gmail.com",
professional_email:"srinivas@dell.com",
temp_email: "temp@dell.com"
},
mob:
{
personal:111111111111,
professional:22222222,
temporary: 3333333333
},
salary: 111111,
depart:"IT Support"
});

db.student.find();



/******************** Install Neo4J Graph DB on ubuntu *****************************/




sudo apt-get update && sudo apt-get upgrade -y

sudo apt-get install wget curl nano software-properties-common dirmngr apt-transport-https gnupg gnupg2 ca-certificates lsb-release ubuntu-keyring unzip -y

curl -fsSL https://debian.neo4j.com/neotechnology.gpg.key | sudo gpg --dearmor -o /usr/share/keyrings/neo4j.gpg

echo "deb [signed-by=/usr/share/keyrings/neo4j.gpg] https://debian.neo4j.com stable latest" | sudo tee -a /etc/apt/sources.list.d/neo4j.list

sudo apt-get update

sudo apt-get install neo4j -y

sudo systemctl enable --now neo4j

sudo nano /etc/neo4j/neo4j.conf

In that file, look for the following line:

#server.default_listen_address=0.0.0.0

Remove the #, so the line now reads:

server.default_listen_address=0.0.0.0

Save and close the file with the CTRL+X keyboard shortcut. Restart the Neo4j service with:

sudo systemctl restart neo4j

You must also edit the system hosts file. To do that, issue the command:

sudo nano /etc/hosts

At the bottom of the file, add a line like this:

SERVER_IP HOSTNAME

Where SERVER_IP is the IP address of the hosting server and HOSTNAME is the hostname of the machine. Save and close the file. For example, if your IP address is 192.168.1.7 and your hostname is fossa, the line would be:

192.168.1.7 fossa

How to test the Neo4j connection
To test the Neo4j connection, the command would look something like this:

cypher-shell -a 'neo4j://192.168.1.7:7687'

Both the default username and password are neo4j. After typing the default password, youâ€™ll be prompted to create a new one. Once youâ€™ve done that, youâ€™ll find yourself at the Neo4j console.

If the connection fails, you might have to open the firewall on the server. To do that, youâ€™ll also want to know the IP address of any machine that will connect to the server. For example, if youâ€™re connecting from IP address 192.168.1.100, you could open the firewall with the command:

sudo ufw allow from 192.168.1.62 to any port 7687 proto tcp

If you want to open the connection to any machine on your network, that command might look like this:

sudo ufw allow from 192.168.1.0/24 to any port 7687 proto tcp



/************************ Install Cassandra *************************/

java -version

sudo apt install apt-transport-https

wget -qO- https://downloads.apache.org/cassandra/KEYS | sudo apt-key add -

echo "deb https://debian.cassandra.apache.org 41x main" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list

sudo apt update

sudo apt install cassandra -y

sudo apt install cassandra -y

sudo systemctl start cassandra

sudo systemctl restart cassandra

sudo systemctl stop cassandra

sudo systemctl start cassandra


cqlsh
help
cqlsh --help				Shows help topics about the options of cqlsh commands.
cqlsh --version			Provides the version of the cqlsh you are using.
cqlsh --color				Directs the shell to use colored output.
cqlsh --debug				Shows additional debugging information.
cqlsh --execute

cql_statement


/**************************************4/7****************************************************/
curl -fsSL https://pgp.mongodb.com/server-8.0.pub | gpg --dearmor | sudo tee /usr/share/keyrings/mongodb-server-8.0.gpg > /dev/null
/*************************. Install MongoDB on Ubuntu ***************************

cat /etc/lsb-release //ubuntu version
sudo apt-get install gnupg curl (Not required)
 	
curl -fsSL https://www.mongodb.org/static/pgp/server-8.0.asc |sudo gpg -o /usr/share/keyrings/mongodb-server-8.0.gpg --dearmor

echo "deb [ arch=amd64,arm64 signed-by=/usr/share/keyrings/mongodb-server-8.0.gpg ] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/8.0 multiverse" | sudo tee /etc/apt/sources.list.d/mongodb-org-8.0.list
   
sudo apt-get update
   
sudo apt-get install -y mongodb-org

sudo systemctl start mongod

sudo systemctl daemon-reload

sudo systemctl status mongod

sudo systemctl enable mongod

sudo systemctl restart mongod

mongosh

help
db.help()
db.tablename.help
db. (tab tab)
dp.tablename. (tab tab)


db.createCollection("incidents");

db.incidents.insert({_id:1, name:"Server Issue"});
db.incidents.insert({_id:2, name:"DB Issue"});
db.incidents.insert({_id:3, name:"Application Issue"});


db.createCollection("databases");

db.incidents.insert({name:"XYZ Issue"});
db.incidents.insert({name:"ABCD Issue"});
db.incidents.insert({name:"MNOP Issue"});


show tables;

#Json Object

Student Collection (Table) 
{
name: "Srinivas",
email: "sriniwas@gmail.com",
mob:9999999999,
salary: 111111,
depart:"IT Support"
}


db.student.insert({
name: "Sriniwas",
email: "sriniwas@gmail.com",
mob:9999999999,
salary: 111111,
depart:"IT Support"
});


;


db.student.insert({
name: "Srinivas",
email:
{
personal_email: "xyz@gmail.com",
professional_email:"srinivas@dell.com",
temp_email: "temp@dell.com"
},
mob:
{
personal:111111111111,
professional:22222222,
temporary: 3333333333
},
salary: 111111,
depart:"IT Support"
});

db.student.find();



/******************** Install Neo4J Graph DB on ubuntu *****************************/


sudo apt-get update && sudo apt-get upgrade -y

sudo apt-get install wget curl nano software-properties-common dirmngr apt-transport-https gnupg gnupg2 ca-certificates lsb-release ubuntu-keyring unzip -y

curl -fsSL https://debian.neo4j.com/neotechnology.gpg.key | sudo gpg --dearmor -o /usr/share/keyrings/neo4j.gpg	

echo "deb [signed-by=/usr/share/keyrings/neo4j.gpg] https://debian.neo4j.com stable latest" | sudo tee -a /etc/apt/sources.list.d/neo4j.list

sudo apt-get update

sudo apt-get install neo4j -y

      sudo systemctl enable --now neo4j

sudo nano /etc/neo4j/neo4j.conf

In that file, look for the following line:

#server.default_listen_address=0.0.0.0

Remove the #, so the line now reads:
exit
server.default_listen_address=0.0.0.0

Save and close the file with the CTRL+X keyboard shortcut. Restart the Neo4j service with:

sudo systemctl restart neo4j

You must also edit the system hosts file. To do that, issue the command:

sudo nano /etc/hosts

At the bottom of the file, add a line like this:

SERVER_IP HOSTNAME

If you get the Java version issue please exposrt the compatible java version
JAVA_HOME=/usr/lib/jvm/java-1.21.0-openjdk-amd64 


Where SERVER_IP is the IP address of the hosting server and HOSTNAME is the hostname of the machine. Save and close the file. For example, if your IP address is 192.168.1.7 and your hostname is fossa, the line would be:
for eg 192.168.1.7 fossa

use command 
hostname (to check the host name)

How to test the Neo4j connection
To test the Neo4j connection, the command would look something like this:

cypher-shell -a 'neo4j://192.168.1.7:7687' (IP address of machine you have installed it)

Both the default username and password are neo4j. After typing the default password, youâ€™ll be prompted to create a new one. Once youâ€™ve done that, youâ€™ll find yourself at the Neo4j console.

If the connection fails, you might have to open the firewall on the server. To do that, youâ€™ll also want to know the IP address of any machine that will connect to the server. For example, if youâ€™re connecting from IP address 192.168.1.100, you could open the firewall with the command:

sudo ufw allow from 192.168.1.62 to any port 7687 proto tcp

If you want to open the connection to any machine on your network, that command might look like this:

sudo ufw allow from 192.168.1.0/24 to any port 7687 proto tcp



/************************ Install Cassandra *************************/

java -version

sudo apt install apt-transport-https

wget -qO- https://downloads.apache.org/cassandra/KEYS | sudo apt-key add -

echo "deb https://debian.cassandra.apache.org 41x main" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list

sudo apt update

sudo apt install cassandra -y

sudo systemctl start cassandra

sudo systemctl restart cassandra

sudo systemctl stop cassandra

sudo systemctl start cassandra

----------------
| Command                 | Purpose                             |                                     |                          |
| ----------------------- | ----------------------------------- | ----------------------------------- | ------------------------ |
| `java -version`         | Check Java version                  |                                     |                          |
| `apt-transport-https`   | Enable secure package download      |                                     |                          |
| `apt-key add`           | Trust Cassandraâ€™s software packages |                                     |                          |
| \`echo â€¦                | tee\`                               | Add Cassandraâ€™s repo to your system |                          |
| `apt update`            | Refresh available software          |                                     |                          |
| `apt install cassandra` | Install Cassandra                   |                                     |                          |
| \`systemctl start       | restart                             | stop\`                              | Manage Cassandra service |
----------------


cqlsh
help
cqlsh --help				Shows help topics about the options of cqlsh commands.
cqlsh --version			Provides the version of the cqlsh you are using.
cqlsh --color				Directs the shell to use colored output.
cqlsh --debug				Shows additional debugging information.
cqlsh --execute

cql_statement

-- Create a Keyspace

CREATE KEYSPACE mykeyspace 
WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};

-- Use the Keyspace
USE mykeyspace;

-- Create a Table
CREATE TABLE users (
  id UUID PRIMARY KEY,
  name TEXT,
  email TEXT,
  age INT
);

INSERT INTO users (id, name, email, age)
VALUES (uuid(), 'John Doe', 'john@example.com', 30);

-- Get all rows
SELECT * FROM users;

-- Get by ID (replace UUID with actual ID)
SELECT * FROM users WHERE id = 2b5b1ae0-4c93-11ee-be56-0242ac120002; (Check you ID)

UPDATE users 
SET age = 31 
WHERE id = 2b5b1ae0-4c93-11ee-be56-0242ac120002;

-- Delete a specific row
DELETE FROM users 
WHERE id = 2b5b1ae0-4c93-11ee-be56-0242ac120002;

--------------
uuid() is a built-in function to generate a random UUID.

In Cassandra, you must specify the full primary key in WHERE clauses for update and delete.

Tables in Cassandra are denormalized and write-optimized, so structure them based on query patterns.

*************************. Install Redis on Ubuntu ***************************
  
sudo apt-get install lsb-release curl gpg
curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg
sudo chmod 644 /usr/share/keyrings/redis-archive-keyring.gpg
echo "deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/redis.list
sudo apt-get update
sudo apt-get install redis
sudo apt install redis-serve	r
redis-cli
  
  PING
  SET mykey somevalue
  GET mykey
  ACL CAT
  INFO SERVER
  5 INCR mycounter
  


redis>JSON.SET doc $ '{"a":2}'
redis> JSON.SET doc $.a '3'
redis> JSON.SET doc $ '{"f1": {"a":1}, "f2":{"a":2}}'
OK
redis> JSON.SET doc $..a 3
OK
redis> JSON.GET doc"{\"f1\":{\"a\":3},\"f2\":{\"a\":3}}"


JSON.SET "bicycle:0" "." "{\"brand\": \"Velorim\", \"model\": \"Jigger\", \"price\": 270, \"description\": \"Small and powerful, the Jigger is the best ride for the smallest of tikes! This is the tiniest kids\\u2019 pedal bike on the market available without a coaster brake, the Jigger is the vehicle of choice for the rare tenacious little rider raring to go.\", \"condition\": \"new\"}"
JSON.SET "bicycle:1" "." "{\"brand\": \"Bicyk\", \"model\": \"Hillcraft\", \"price\": 1200, \"description\": \"Kids want to ride with as little weight as possible. Especially on an incline! They may be at the age when a 27.5\\\" wheel bike is just too clumsy coming off a 24\\\" bike. The Hillcraft 26 is just the solution they need!\", \"condition\": \"used\"}"
JSON.SET "bicycle:2" "." "{\"brand\": \"Nord\", \"model\": \"Chook air 5\", \"price\": 815, \"description\": \"The Chook Air 5  gives kids aged six years and older a durable and uberlight mountain bike for their first experience on tracks and easy cruising through forests and fields. The lower  top tube makes it easy to mount and dismount in any situation, giving your kids greater safety on the trails.\", \"condition\": \"used\"}"
JSON.SET "bicycle:3" "." "{\"brand\": \"Eva\", \"model\": \"Eva 291\", \"price\": 3400, \"description\": \"The sister company to Nord, Eva launched in 2005 as the first and only women-dedicated bicycle brand. Designed by women for women, allEva bikes are optimized for the feminine physique using analytics from a body metrics database. If you like 29ers, try the Eva 291. It\\u2019s a brand new bike for 2022.. This full-suspension, cross-country ride has been designed for velocity. The 291 has 100mm of front and rear travel, a superlight aluminum frame and fast-rolling 29-inch wheels. Yippee!\", \"condition\": \"used\"}"
JSON.SET "bicycle:4" "." "{\"brand\": \"Noka Bikes\", \"model\": \"Kahuna\", \"price\": 3200, \"description\": \"Whether you want to try your hand at XC racing or are looking for a lively trail bike that's just as inspiring on the climbs as it is over rougher ground, the Wilder is one heck of a bike built specifically for short women. Both the frames and components have been tweaked to include a women\\u2019s saddle, different bars and unique colourway.\", \"condition\": \"used\"}"
JSON.SET "bicycle:5" "." "{\"brand\": \"Breakout\", \"model\": \"XBN 2.1 Alloy\", \"price\": 810, \"description\": \"The XBN 2.1 Alloy is our entry-level road bike \\u2013 but that\\u2019s not to say that it\\u2019s a basic machine. With an internal weld aluminium frame, a full carbon fork, and the slick-shifting Claris gears from Shimano\\u2019s, this is a bike which doesn\\u2019t break the bank and delivers craved performance.\", \"condition\": \"new\"}"
JSON.SET "bicycle:6" "." "{\"brand\": \"ScramBikes\", \"model\": \"WattBike\", \"price\": 2300, \"description\": \"The WattBike is the best e-bike for people who still feel young at heart. It has a Bafang 1000W mid-drive system and a 48V 17.5AH Samsung Lithium-Ion battery, allowing you to ride for more than 60 miles on one charge. It\\u2019s great for tackling hilly terrain or if you just fancy a more leisurely ride. With three working modes, you can choose between E-bike, assisted bicycle, and normal bike modes.\", \"condition\": \"new\"}"
JSON.SET "bicycle:7" "." "{\"brand\": \"Peaknetic\", \"model\": \"Secto\", \"price\": 430, \"description\": \"If you struggle with stiff fingers or a kinked neck or back after a few minutes on the road, this lightweight, aluminum bike alleviates those issues and allows you to enjoy the ride. From the ergonomic grips to the lumbar-supporting seat position, the Roll Low-Entry offers incredible comfort. The rear-inclined seat tube facilitates stability by allowing you to put a foot on the ground to balance at a stop, and the low step-over frame makes it accessible for all ability and mobility levels. The saddle is very soft, with a wide back to support your hip joints and a cutout in the center to redistribute that pressure. Rim brakes deliver satisfactory braking control, and the wide tires provide a smooth, stable ride on paved roads and gravel. Rack and fender mounts facilitate setting up the Roll Low-Entry as your preferred commuter, and the BMX-like handlebar offers space for mounting a flashlight, bell, or phone holder.\", \"condition\": \"new\"}"
JSON.SET "bicycle:8" "." "{\"brand\": \"nHill\", \"model\": \"Summit\", \"price\": 1200, \"description\": \"This budget mountain bike from nHill performs well both on bike paths and on the trail. The fork with 100mm of travel absorbs rough terrain. Fat Kenda Booster tires give you grip in corners and on wet trails. The Shimano Tourney drivetrain offered enough gears for finding a comfortable pace to ride uphill, and the Tektro hydraulic disc brakes break smoothly. Whether you want an affordable bike that you can take to work, but also take trail in mountains on the weekends or you\\u2019re just after a stable, comfortable ride for the bike path, the Summit gives a good value for money.\", \"condition\": \"new\"}"
JSON.SET "bicycle:9" "." "{\"model\": \"ThrillCycle\", \"brand\": \"BikeShind\", \"price\": 815, \"description\": \"An artsy,  retro-inspired bicycle that\\u2019s as functional as it is pretty: The ThrillCycle steel frame offers a smooth ride. A 9-speed drivetrain has enough gears for coasting in the city, but we wouldn\\u2019t suggest taking it to the mountains. Fenders protect you from mud, and a rear basket lets you transport groceries, flowers and books. The ThrillCycle comes with a limited lifetime warranty, so this little guy will last you long past graduation.\", \"condition\": \"refurbished\"}"


KEYS *

keys d??

keys *t*

KEYS "prefix:*"


MSET firstname Jack lastname Stuntman age 35
KEYS *name*
KEYS a??
KEYS *

Redis databases are numbered from 0 to 15 and, 
by default, you connect to database 0 when you connect to your Redis instance. 
However, you can change the database youâ€™re using with the select command after you connect:

To swap all the data held in one database with the data held in another, use the swapdb command. 
The following example will swap the data held in database 6 with the data in database 8, and any clients connected to either database 
will be able to implement changes immediately:

swapdb 6 8

There are a number of Redis commands that are useful for managing keys regardless of what type of data they hold. Some of these commands are reviewed in the following section.


rename old_key new_key

You can use randomkey to return a random key from the currently selected database:

randomkey



Deleting Keys
To delete one or more keys of any data type, use the del command followed by one or more keys that you want to delete:

del key_1 key_2

To delete all the keys in the selected database, use the flushdb command:


flushdb

To delete all the keys in every database on a Redis server (including the currently selected database), run flushall:

flushall

Backing Up Your Database
To create a backup of the currently selected database, you can use the save command:

save

shutdown
shutdown save
shutdown nosave



Treat a list like a queue (first in, first out):

> LPUSH bikes:repairs bike:1
> LPUSH bikes:repairs bike:2
> RPOP bikes:repairs
> RPOP bikes:repairs


> LPUSH bikes:repairs bike:1
> LPUSH bikes:repairs bike:2
> LPOP bikes:repairs
> LPOP bikes:repairs

> LLEN bikes:repairs
> LPUSH bikes:repairs bike:1
> LPUSH bikes:repairs bike:2
> LMOVE bikes:repairs bikes:finished LEFT LEFT
> LRANGE bikes:repairs 0 -1
> LRANGE bikes:finished 0 -1


> LPUSH bikes:repairs bike:1
> LPUSH bikes:repairs bike:2
> LMOVE bikes:repairs bikes:finished LEFT LEFT
> LRANGE bikes:repairs 0 -1
> LRANGE bikes:finished 0 -1


> RPUSH bikes:repairs bike:1
> RPUSH bikes:repairs bike:2
> LPUSH bikes:repairs bike:important_bike
> LRANGE bikes:repairs 0 -1

******************************************* Docker *******************************************

To check if Docker is installed on your local Ubuntu OS, open the terminal and run:

docker --version

To see the containers currently running on Docker, use the following command in your terminal:

docker ps

docker ps -a


| Command                         | Description                        |
| ------------------------------- | ---------------------------------- |
| `sudo systemctl start docker`   | Starts Docker service              |
| `sudo systemctl restart docker` | Restarts Docker service            |
| `sudo systemctl status docker`  | Shows Docker service status        |
| `sudo systemctl disable docker` | Disables Docker from auto-starting |
| `sudo systemctl enable docker`  | Enables Docker to start on boot    |

sudo systemctl status docker

--To come out press Ctl + C
--If Docker is not running 

sudo systemctl start docker

---sudo systemctl restart docker

---sudo systemctl disable docker
---sudo systemctl enable docker


/********************************************************7/7*********************************************/
On Ubuntu 

Go to spark directory 

cd spark 

Check the current working directory 

pwd

output: /home/ubuntu/spark

# Download the iceberg compatible with Spark version we have spark 3.5.6 in the lab 

wget https://repo.maven.apache.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.5.0/iceberg-spark-runtime-3.5_2.12-1.5.0.jar

# Check if the Iceberg is downloaded 

ls -lh iceberg-spark-runtime-3.5_2.12-1.5.0.jar
-rw-rw-r-- 1 ubuntu ubuntu 40M Mar  5  2024 iceberg-spark-runtime-3.5_2.12-1.5.0.jar

If you get any issue change the mode of the iceberg jar

chmod 777 iceberg-spark-runtime-3.5_2.12-1.5.0.jar 

This will set the hadoop as catalog 
  
------

spark-sql \
  --jars /home/ubuntu/spark/iceberg-spark-runtime-3.5_2.12-1.5.0.jar \
  --conf spark.sql.catalog.iceberg_catalog=org.apache.iceberg.spark.SparkCatalog \
  --conf spark.sql.catalog.iceberg_catalog.type=hadoop \
  --conf spark.sql.catalog.iceberg_catalog.warehouse=file:///home/ubuntu/spark/warehouse \
  --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions

  
# The warnings youâ€™re seeing are common and non-blocking in most local setups.

#you would get the prompt: spark-sql (default)>

# How to create a databases?

create database mydb;
use mydb;

# How to list databases?

show databases;



Create a table:

CREATE TABLE spark_catalog.default.employee (
  id INT,
  name STRING,
  department STRING,
  salary INT
)
USING iceberg;

INSERT INTO spark_catalog.default.employee VALUES
  (201, 'Anya', 'HR', 75000),
  (202, 'Ben', 'Finance', 82000),
  (203, 'Cara', 'Engineering', 95000);


SELECT * FROM spark_catalog.default.employee;

UPDATE spark_catalog.default.employee
SET salary = salary + 10000
WHERE department = 'Engineering';

#check if the changes have taken place 

SELECT * FROM spark_catalog.default.employee;


DELETE FROM spark_catalog.default.employee
WHERE department = 'HR';

Check if HR is deleted 
SELECT * FROM spark_catalog.default.employee;


#SELECT * FROM spark_catalog.default.employee WHERE salary > 80000;

#First, check all snapshots:

CALL spark_catalog.system.snapshots('default.employee');

#if it does not work try the below if the call function is not found 

spark-sql (default)> SELECT * FROM iceberg_catalog.default.employee.snapshots;

#Output will be 
*********************
2025-07-05 13:50:44.071	554849194851258724	NULL	append	file:/home/ubuntu/spark/warehouse/default/employee/metadata/snap-554849194851258724-1-cca1a627-b9f5-488b-a81e-c1296bc0cd2e.avro	{"added-data-files":"2","added-files-size":"2184","added-records":"3","changed-partition-count":"1","spark.app.id":"local-1751721654277","total-data-files":"2","total-delete-files":"0","total-equality-deletes":"0","total-files-size":"2184","total-position-deletes":"0","total-records":"3"}
2025-07-05 14:01:49.186	3033299854398411860	554849194851258724	overwrite	file:/home/ubuntu/spark/warehouse/default/employee/metadata/snap-3033299854398411860-1-b3be4655-9161-46b6-a528-3b45d0b38721.avro	{"added-data-files":"1","added-files-size":"1147","added-records":"2","changed-partition-count":"1","deleted-data-files":"1","deleted-records":"2","removed-files-size":"1112","spark.app.id":"local-1751721654277","total-data-files":"2","total-delete-files":"0","total-equality-deletes":"0","total-files-size":"2219","total-position-deletes":"0","total-records":"3"}
2025-07-05 14:04:21.535	5308826326954041526	3033299854398411860	delete	file:/home/ubuntu/spark/warehouse/default/employee/metadata/snap-5308826326954041526-1-772cbe65-d8ea-4785-a4d0-50edeb4f2cc5.avro	{"changed-partition-count":"1","deleted-data-files":"1","deleted-records":"1","removed-files-size":"1072","spark.app.id":"local-1751721654277","total-data-files":"1","total-delete-files":"0","total-equality-deletes":"0","total-files-size":"1147","total-position-deletes":"0","total-records":"2"}
Time taken: 0.334 seconds, Fetched 3 row(s)

Explanation of output: 

| **Timestamp**         | **Snapshot ID**       | **Operation** | **Parent**            | **What Happened**                        |
| --------------------- | --------------------- | ------------- | --------------------- | ---------------------------------------- |
| `2025-07-05 13:50:44` | `554849194851258724`  | `append`      | `NULL`                | Added 3 records in 2 data files          |
| `2025-07-05 14:01:49` | `3033299854398411860` | `overwrite`   | `554849194851258724`  | Removed 1 data file, added 2 new records |
| `2025-07-05 14:04:21` | `5308826326954041526` | `delete`      | `3033299854398411860` | Deleted 1 record and 1 data file         |
**********************
#Query a past snapshot

SELECT * FROM iceberg_catalog.default.employee 
VERSION AS OF 3033299854398411860;

********************** the output shows all the records but we had deleted the employee HR**********

202	Ben	Finance	82000
203	Cara	Engineering	105000
201	Anya	HR	75000
Time taken: 1.019 seconds, Fetched 3 row(s)

#Query by timestamp specify your timestamp 

SELECT * FROM iceberg_catalog.default.employee 
TIMESTAMP AS OF '2025-07-05 14:01:49';

#Rollback to an earlier snapshot (if needed specify the ID you have)

CALL iceberg_catalog.system.rollback_to_snapshot(
  'default.employee', 
  3033299854398411860
);

#Output 
5308826326954041526	3033299854398411860
Time taken: 0.141 seconds, Fetched 1 row(s)

SELECT * FROM iceberg_catalog.default.employee;

#You should now see the records that existed right after the overwrite and before the delete.
201	Anya	HR	75000
202	Ben	Finance	82000
203	Cara	Engineering	105000

*****************

SELECT * FROM iceberg_catalog.default.employee.snapshots;

# Even though the rollback was executed, the number of snapshots is still 3 â€” and there's no new snapshot yet.

2025-07-05 13:50:44.071	554849194851258724	NULL	append	file:/home/ubuntu/spark/warehouse/default/employee/metadata/snap-554849194851258724-1-cca1a627-b9f5-488b-a81e-c1296bc0cd2e.avro	{"added-data-files":"2","added-files-size":"2184","added-records":"3","changed-partition-count":"1","spark.app.id":"local-1751721654277","total-data-files":"2","total-delete-files":"0","total-equality-deletes":"0","total-files-size":"2184","total-position-deletes":"0","total-records":"3"}
2025-07-05 14:01:49.186	3033299854398411860	554849194851258724	overwrite	file:/home/ubuntu/spark/warehouse/default/employee/metadata/snap-3033299854398411860-1-b3be4655-9161-46b6-a528-3b45d0b38721.avro	{"added-data-files":"1","added-files-size":"1147","added-records":"2","changed-partition-count":"1","deleted-data-files":"1","deleted-records":"2","removed-files-size":"1112","spark.app.id":"local-1751721654277","total-data-files":"2","total-delete-files":"0","total-equality-deletes":"0","total-files-size":"2219","total-position-deletes":"0","total-records":"3"}
2025-07-05 14:04:21.535	5308826326954041526	3033299854398411860	delete	file:/home/ubuntu/spark/warehouse/default/employee/metadata/snap-5308826326954041526-1-772cbe65-d8ea-4785-a4d0-50edeb4f2cc5.avro	{"changed-partition-count":"1","deleted-data-files":"1","deleted-records":"1","removed-files-size":"1072","spark.app.id":"local-1751721654277","total-data-files":"1","total-delete-files":"0","total-equality-deletes":"0","total-files-size":"1147","total-position-deletes":"0","total-records":"2"}


********** Imp Point ********
Why No New Snapshot After Rollback?
Iceberg does not create a new snapshot just for rollback. Instead, it changes the current metadata pointer to point to the previous snapshot. So the rollback is immediate, but doesn't leave an audit trail unless you make a new data change.

This is expected behavior.
*****************************

SELECT snapshot_id, parent_id, operation, summary
FROM iceberg_catalog.default.employee.snapshots
ORDER BY committed_at;

#output
554849194851258724	NULL	append	{"added-data-files":"2","added-files-size":"2184","added-records":"3","changed-partition-count":"1","spark.app.id":"local-1751721654277","total-data-files":"2","total-delete-files":"0","total-equality-deletes":"0","total-files-size":"2184","total-position-deletes":"0","total-records":"3"}
3033299854398411860	554849194851258724	overwrite	{"added-data-files":"1","added-files-size":"1147","added-records":"2","changed-partition-count":"1","deleted-data-files":"1","deleted-records":"2","removed-files-size":"1112","spark.app.id":"local-1751721654277","total-data-files":"2","total-delete-files":"0","total-equality-deletes":"0","total-files-size":"2219","total-position-deletes":"0","total-records":"3"}
5308826326954041526	3033299854398411860	delete	{"changed-partition-count":"1","deleted-data-files":"1","deleted-records":"1","removed-files-size":"1072","spark.app.id":"local-1751721654277","total-data-files":"1","total-delete-files":"0","total-equality-deletes":"0","total-files-size":"1147","total-position-deletes":"0","total-records":"2"}

if the otput is very verbose 

spark-sql \
  --conf "spark.sql.catalog.iceberg_catalog=org.apache.iceberg.spark.SparkCatalog" \
  --conf "spark.sql.catalog.iceberg_catalog.type=hadoop" \
  --conf "spark.sql.catalog.iceberg_catalog.warehouse=file:///home/ubuntu/spark/warehouse" \
  --conf "spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions" \
  --conf "spark.executor.extraJavaOptions=-Dlog4j.configuration=log4j.properties" \
  --conf "spark.driver.extraJavaOptions=-Dlog4j.configuration=log4j.properties"
  


