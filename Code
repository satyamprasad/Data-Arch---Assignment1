generate synthetic data using faker library for order management for supply chain management use case which is designed for a company which manufactures laptops, servers, peripherals. this data should have the order id quantity make and model, brand, Quantity, Unit Price, Total Price etc
https://gist.githubusercontent.com/provpup/2fc41686eab7400b796b/raw/b575bd01a58494dfddc1d6429ef0167e709abf9b/hamlet.txt
/***23 June****/


Making a list parallelized
rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])

#To Show all the elements 
rdd.collect() 

# First 3 elements
rdd.take(3)          

# Number of elements
rdd.count()         

# First element
rdd.first()          

#Flat Map
rdd_flatmap = rdd.flatMap(lambda x: (x, x*10))
rdd_flatmap.collect()

# Reduce to a single value by adding each element of the list 
rdd.reduce(lambda x, y: x + y)  

#Multiplying every element by 2 or any number and show the result 
rdd_map = rdd.map(lambda x: x * 2)
rdd_map.collect()

#Filter list value which is even number
rdd_filter = rdd.filter(lambda x: x % 2 == 0)
rdd_filter.collect()

Create a Data Frame:

#Creating Data Frame using range function 

df_range = spark.range(10)  
df_range.show()

#Creating Data Frame using your dataset and defining the Data Frame and show the result 

data = [("Mark", 35), ("Bob", 25), ("Charlie", 38)]
df = spark.createDataFrame(data, ["Name", "Age"])
df.show()

df_selected = df.select("Name")
df_selected.show()

df_multi_select = df.select("Name", "Age")

df_multi_select.show()


from pyspark.sql.functions import col (if you get issue)

# Now your filter command will work 
df_filtered = df.filter(col("Age") > 1)
df_filtered.show()

# Using SQL-like syntax
df_filtered_sql = df.filter("Age > 1")
df_filtered_sql.show()

#Define a alias
df_multi_select = df.select(col("Name"), col("Age").alias("YearsOld"))
df_multi_select.show()

Add a new column:
from pyspark.sql.functions import lit 

df_with_col = df.withColumn("City", lit("New York"))
df_with_col.show()

Doing calculation on column:

df_calc_col = df.withColumn("AgeInMonths", col("Age") * 12)
df_calc_col.show()

Drop a column:
df_no_age = df.drop("Age")
df_no_age.show()

Group by and aggregate:

# Assuming df_sales has 'ProductType' and 'Revenue'
# For our simple df, let's pretend 'Name' is product type and 'Age' is quantity

df_agg = df.groupBy("Name").agg({"Age": "sum"})
df_agg.show()

from pyspark.sql.functions import sum, avg
df_agg_multi = df.groupBy("Name").agg(sum("Age").alias("TotalQuantity"), avg("Age").alias("AvgQuantity"))
df_agg_multi.show()

Sort (Order By):
df_sorted = df.orderBy("Age", ascending=False)
df_sorted.show()

#Reading a CSV file (assuming path and file is accessible from Spark):
df_csv = spark.read.csv("path/to/your/file.csv", header=True, inferSchema=True)
df_csv.show(5)

df_csv.show() # Shows first 20 rows by default
df_csv.show(5, truncate=False) # Show first 5 rows, don't truncate long strings

Count rows:
row_count = df_csv.count()
print(f"Number of rows: {row_count}")

Collect data (bring to driver as Python list):
all_data = df_csv.collect()
for row in all_data:
    print(row)

(*Use collect() with caution on very large DataFrames, as it brings all data to the driver's memory.)
Take a sample of rows:

sample_rows = df_csv.take(2)
print(sample_rows)
Write to file:
df.write.csv("my_data_output.csv", header=True, mode="overwrite")
df.write.parquet("my_data_output.parquet", mode="overwrite")

(mode="overwrite" will overwrite if the directory already exists. These files will be created in very small pieces. How can we make one file?)
It will create a folder with many files 


#Reading a Parquet file:
df_parquet = spark.read.parquet("path/to/your/data.parquet")
df_parquet.show(5)


When you read this file it will give the proper output so breaking the files will be internal to spark.
Print schema:

df_parquet.printSchema()
Describe (summary statistics):
df.describe().show()
# Assuming df_csv is your DataFrame
df_csv.describe().show(truncate=False)

from pyspark.sql.functions import col

# Only describe numeric columns
df_csv.select("Quantity", "Unit Price", "Total Price").describe().show()


pandas_df_description = df_csv.describe().toPandas()

print(pandas_df_description)

Use toPandas() with extreme caution on very large Spark DataFrames, as it pulls ALL data into your driver's memory. However, for the output of describe(), which is usually small (only a few rows), it's generally safe.



/************************** Spark SQL **************************/


Spark SQL is Apache Sparkâ€™s module for working with structured data. The SQL Syntax section describes the SQL syntax in detail along with usage examples when applicable. This document provides a list of Data Definition and Data Manipulation Statements, as well as Data Retrieval and Auxiliary Statements.
DDL Statements
Data Definition Statements are used to create or modify the structure of database objects in a database. Spark SQL supports the following Data Definition Statements:

CREATE DATABASE
CREATE FUNCTION
CREATE TABLE
CREATE VIEW
ALTER DATABASE
ALTER TABLE
ALTER VIEW
DECLARE VARIABLE
DROP DATABASE
DROP FUNCTION
DROP TABLE
DROP TEMPORARY VARIABLE
DROP VIEW
REPAIR TABLE
TRUNCATE TABLE
USE DATABASE
DML Statements

Data Manipulation Statements are used to add, change, or delete data. Spark SQL supports the following Data Manipulation Statements:
INSERT TABLE
INSERT OVERWRITE DIRECTORY
LOAD

Data Retrieval StatementsSpark supports 

SELECT statement that is used to retrieve rows from one or more tables according to the specified clauses. The full syntax and brief description of supported clauses are explained in the SELECT section. The SQL statements related to SELECT are also included in this section. Spark also provides the ability to generate logical and physical plan for a given query using EXPLAIN statement.

SELECT Statement

SHOW COLUMNS
SHOW CREATE TABLE
SHOW DATABASES
SHOW FUNCTIONS
SHOW PARTITIONS
SHOW TABLE EXTENDED
SHOW TABLES
SHOW TBLPROPERTIES
SHOW VIEWS
UNCACHE TABLE

*******************************************

Read a text file and count the words it has and print the count on console
**************************************************************************

from pyspark import SparkContext

# Step 1: Initialize Spark Context
sc = SparkContext(appName="WordFrequencyCountRDD")

# Step 2: Load the text file
rdd = sc.textFile("/path/to/your/textfile.txt")  #  Replace with actual file path

# Step 3: Split lines into words
words = rdd.flatMap(lambda line: line.lower().split())

# Step 4: Map each word to a (word, 1) tuple
word_pairs = words.map(lambda word: (word, 1))

# Step 5: Reduce by key (sum up all 1s per word
word_counts = word_pairs.reduceByKey(lambda a, b: a + b)

# Step 6: Collect and print the result
for word, count in word_counts.collect():
    print(f"{word}: {count}")


# Step 7: Stop Spark context
sc.stop()

from pyspark.sql import SparkSession, Row

# Step 1: Start Spark session
spark = SparkSession.builder.appName("WordCountWithStopwords").getOrCreate()
sc = spark.sparkContext

# Step 2: Define stopwords (you can expand this list)
stopwords = {"to", "if", "the", "in", "for", "and", "on", "be", "a", "an", "of", "with", "at", "by", "is", "it", "this"}

# Step 3: Read input text file
rdd = sc.textFile("/Users/alokranjan/input.txt")

# Step 4: Tokenize and clean
words = rdd.flatMap(lambda line: line.lower().split()) \
           .filter(lambda word: word.isalpha() and word not in stopwords)

# Step 5: Word count
word_counts = words.map(lambda word: (word, 1)) \
                   .reduceByKey(lambda a, b: a + b)

# Step 6: Convert to DataFrame
word_df = word_counts.map(lambda x: Row(word=x[0], count=x[1])).toDF()

# Optional: Show result
word_df.orderBy("count", ascending=False).show(20)

# Optional: Save to file
word_df.write.mode("overwrite").option("header", True).csv("/Users/alokranjan/cleaned_word_count_csv")

# Step 7: Stop Spark session
spark.stop()

*****************************************************************
Plot a histogram 
*****************************************************************
import matplotlib.pyplot as plt

# Convert Spark DataFrame to Pandas
pandas_df = word_df.toPandas()

# Sort by count (optional)
pandas_df = pandas_df.sort_values(by="count", ascending=False)

# Plot histogram
plt.figure(figsize=(10, 6))
plt.bar(pandas_df["word"], pandas_df["count"], color='skyblue')
plt.xlabel("Words")	
plt.ylabel("Frequency")
plt.title("Word Frequency Histogram")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

##############################2/7########################################
/*************************. Install MongoDB on Ubuntu ***************************

  cat /etc/lsb-release
  sudo apt-get install gnupg curl (Not required)
 	
  curl -fsSL https://www.mongodb.org/static/pgp/server-8.0.asc |sudo gpg -o /usr/share/keyrings/mongodb-server-8.0.gpg --dearmor
 
   echo "deb [ arch=amd64,arm64 signed-by=/usr/share/keyrings/mongodb-server-8.0.gpg ] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/8.0 multiverse" | sudo tee /etc/apt/sources.list.d/mongodb-org-8.0.list
   
   sudo apt-get update
   
   sudo apt-get install -y mongodb-org

sudo systemctl start mongod

sudo systemctl daemon-reload

sudo systemctl status mongod

sudo systemctl enable mongod

sudo systemctl restart mongod

mongosh

help
db.help()
db.tablename.help
db. (tab tab)
dp.tablename. (tab tab)


db.createCollection("incidents");

db.incidents.insert({_id:1, name:"Server Issue"});
db.incidents.insert({_id:2, name:"DB Issue"});
db.incidents.insert({_id:3, name:"Application Issue"});


db.createCollection("databases");

db.incidents.insert({name:"XYZ Issue"});
db.incidents.insert({name:"ABCD Issue"});
db.incidents.insert({name:"MNOP Issue"});


show tables;

#Json Object

Student Collection (Table) 

{
name: "Srinivas",
email: "sriniwas@gmail.com",
mob:9999999999,
salary: 111111,
depart:"IT Support"
}


db.student.insert({
name: "Sivas",
email: "sriniwas@gmail.com",
mob:9999999999,
salary: 111111,
depart:"IT Support"
});


;


db.student.insert({
name: "Srinivas",
email:
{
personal_email: "xyz@gmail.com",
professional_email:"srinivas@dell.com",
temp_email: "temp@dell.com"
},
mob:
{
personal:111111111111,
professional:22222222,
temporary: 3333333333
},
salary: 111111,
depart:"IT Support"
});

db.student.find();



/******************** Install Neo4J Graph DB on ubuntu *****************************/




sudo apt-get update && sudo apt-get upgrade -y

sudo apt-get install wget curl nano software-properties-common dirmngr apt-transport-https gnupg gnupg2 ca-certificates lsb-release ubuntu-keyring unzip -y

curl -fsSL https://debian.neo4j.com/neotechnology.gpg.key | sudo gpg --dearmor -o /usr/share/keyrings/neo4j.gpg

echo "deb [signed-by=/usr/share/keyrings/neo4j.gpg] https://debian.neo4j.com stable latest" | sudo tee -a /etc/apt/sources.list.d/neo4j.list

sudo apt-get update

sudo apt-get install neo4j -y

sudo systemctl enable --now neo4j

sudo nano /etc/neo4j/neo4j.conf

In that file, look for the following line:

#server.default_listen_address=0.0.0.0

Remove the #, so the line now reads:

server.default_listen_address=0.0.0.0

Save and close the file with the CTRL+X keyboard shortcut. Restart the Neo4j service with:

sudo systemctl restart neo4j

You must also edit the system hosts file. To do that, issue the command:

sudo nano /etc/hosts

At the bottom of the file, add a line like this:

SERVER_IP HOSTNAME

Where SERVER_IP is the IP address of the hosting server and HOSTNAME is the hostname of the machine. Save and close the file. For example, if your IP address is 192.168.1.7 and your hostname is fossa, the line would be:

192.168.1.7 fossa

How to test the Neo4j connection
To test the Neo4j connection, the command would look something like this:

cypher-shell -a 'neo4j://192.168.1.7:7687'

Both the default username and password are neo4j. After typing the default password, youâ€™ll be prompted to create a new one. Once youâ€™ve done that, youâ€™ll find yourself at the Neo4j console.

If the connection fails, you might have to open the firewall on the server. To do that, youâ€™ll also want to know the IP address of any machine that will connect to the server. For example, if youâ€™re connecting from IP address 192.168.1.100, you could open the firewall with the command:

sudo ufw allow from 192.168.1.62 to any port 7687 proto tcp

If you want to open the connection to any machine on your network, that command might look like this:

sudo ufw allow from 192.168.1.0/24 to any port 7687 proto tcp



/************************ Install Cassandra *************************/

java -version

sudo apt install apt-transport-https

wget -qO- https://downloads.apache.org/cassandra/KEYS | sudo apt-key add -

echo "deb https://debian.cassandra.apache.org 41x main" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list

sudo apt update

sudo apt install cassandra -y

sudo apt install cassandra -y

sudo systemctl start cassandra

sudo systemctl restart cassandra

sudo systemctl stop cassandra

sudo systemctl start cassandra


cqlsh
help
cqlsh --help				Shows help topics about the options of cqlsh commands.
cqlsh --version			Provides the version of the cqlsh you are using.
cqlsh --color				Directs the shell to use colored output.
cqlsh --debug				Shows additional debugging information.
cqlsh --execute

cql_statement

