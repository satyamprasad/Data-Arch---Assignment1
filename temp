CONTAINER ID   IMAGE                         COMMAND                  CREATED          STATUS                      PORTS                                                                                                                                                           NAMES
215251f21cf5   tabulario/spark-iceberg       "/bin/sh -c ' sleep …"   59 minutes ago   Exited (0) 58 minutes ago                                                                                                                                                                   spark-sql
6cb171848a48   tabulario/spark-iceberg       "./entrypoint.sh not…"   59 minutes ago   Up 59 minutes               0.0.0.0:8888->8888/tcp, :::8888->8888/tcp, 0.0.0.0:10000-10001->10000-10001/tcp, :::10000-10001->10000-10001/tcp, 0.0.0.0:8088->8080/tcp, [::]:8088->8080/tcp   spark-iceberg
14be4113d0bf   minio/mc:latest               "/bin/sh -c ' until …"   59 minutes ago   Up 59 minutes                                                                                                                                                                               mc
7a99504cf64b   naushadh/hive-metastore       "./run.sh"               59 minutes ago   Up 59 minutes (healthy)     0.0.0.0:9083->9083/tcp, :::9083->9083/tcp                                                                                                                       hive-metastore
b3781c0c68ac   tabulario/iceberg-rest        "java -jar iceberg-r…"   59 minutes ago   Up 59 minutes               0.0.0.0:8181->8181/tcp, :::8181->8181/tcp                                                                                                                       iceberg-rest
983784b02919   projectnessie/nessie:latest   "/usr/local/s2i/run"     59 minutes ago   Up 59 minutes               8080/tcp, 8443/tcp, 0.0.0.0:19120->19120/tcp, :::19120->19120/tcp                                                                                               nessie
f1d15121b2ec   minio/minio                   "/usr/bin/docker-ent…"   59 minutes ago   Up 59 minutes               0.0.0.0:9000-9001->9000-9001/tcp, :::9000-9001->9000-9001/tcp                                                                                                   minio
22b69749d178   postgres:14                   "docker-entrypoint.s…"   59 minutes ago   Up 59 minutes               0.0.0.0:5433->5432/tcp, [::]:5433->5432/tcp    



https://repo1.maven.org/maven2/org/apache/hive/hive-jdbc/2.3.9/hive-jdbc-2.3.9-standalone.jar



ubuntu@ip-172-31-10-68:~/spark-3.5.6-bin-hadoop3$ ~/spark-3.5.6-bin-hadoop3/bin/pyspark   --jars ~/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar   --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions   --conf spark.sql.catalog.vendor_catalog=org.apache.iceberg.spark.SparkCatalog   --conf spark.sql.catalog.vendor_catalog.type=hadoop   --conf spark.sql.catalog.vendor_catalog.warehouse=/tmp/warehouse
Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
25/09/16 09:23:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.5.6
      /_/

Using Python version 3.10.12 (main, Aug 15 2025 14:32:43)
Spark context Web UI available at http://ip-172-31-10-68.ap-south-1.compute.internal:4040
Spark context available as 'sc' (master = local[*], app id = local-1758014604345).
SparkSession available as 'spark'.
>>> spark.sql("select * from vendor_catalog.vendor_db.demo").show()
+---+----+                                                                      
| id|name|
+---+----+
|  1|John|
|  2|Alka|
+---+----+

from pyspark.sql import SparkSession
import csv
import sys

# Create SparkSession with Iceberg catalog
spark = SparkSession.builder \
    .appName("StandaloneIcebergCSV") \
    .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
    .config("spark.sql.catalog.vendor_catalog", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.vendor_catalog.type", "hadoop") \
    .config("spark.sql.catalog.vendor_catalog.warehouse", "/tmp/warehouse") \
    .getOrCreate()

# Query your table
df = spark.sql("SELECT * FROM vendor_catalog.vendor_db.demo")

# Write CSV lines to stdout
writer = csv.writer(sys.stdout)
writer.writerow(df.columns)  # header
for row in df.toLocalIterator():
    writer.writerow([row[c] for c in df.columns])

CREATE TABLE vendor_catalog.vendor_db.demo2 (
  id INT,
  name STRING
) USING ICEBERG
LOCATION 'hdfs:///tmp/warehouse/vendor_db/demo2';


/home/ubuntu/spark-3.5.6-bin-hadoop3/bin/spark-submit \
  --jars /home/ubuntu/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar \
  --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
  --conf spark.sql.catalog.vendor_catalog=org.apache.iceberg.spark.SparkCatalog \
  --conf spark.sql.catalog.vendor_catalog.type=hadoop \
  --conf spark.sql.catalog.vendor_catalog.warehouse=/tmp/warehouse \
  /home/ubuntu/nifi/scripts/run_iceberg_query.py


~/spark-3.5.6-bin-hadoop3/bin/spark-submit \
  --jars ~/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar \
  ~/nifi/scripts/run_iceberg_query.py

~/spark-3.5.6-bin-hadoop3/bin/pyspark \
  --jars ~/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar \
  --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
  --conf spark.sql.catalog.vendor_catalog=org.apache.iceberg.spark.SparkCatalog \
  --conf spark.sql.catalog.vendor_catalog.type=hadoop \
  --conf spark.sql.catalog.vendor_catalog.warehouse=/tmp/warehouse

ubuntu@ip-172-31-4-241:~$ ~/spark-3.5.6-bin-hadoop3/bin/pyspark \
  --jars ~/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.4_2.12-1.4.2.jar \
  --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
  --conf spark.sql.catalog.vendor_catalog=org.apache.iceberg.spark.SparkCatalog \
  --conf spark.sql.catalog.vendor_catalog.type=hadoop \
  --conf spark.sql.catalog.vendor_catalog.warehouse=/tmp/warehouse
Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
25/09/20 11:34:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/09/20 11:34:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/09/20 11:34:16 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.5.6
      /_/

Using Python version 3.10.12 (main, Aug 15 2025 14:32:43)
Spark context Web UI available at http://ip-172-31-4-241.ap-south-1.compute.internal:4042
Spark context available as 'sc' (master = local[*], app id = local-1758368056336).
SparkSession available as 'spark'.
>>> spark.sql("create table vendor_catalog.vendor_db.demo(lh_id INT, id INT, name STRING) USING iceberg");
DataFrame[]
>>> spark.sql("select * from vendor_catalog.vendor_db.demo");
DataFrame[lh_id: int, id: int, name: string]
>>> spark.sql("select * from vendor_catalog.vendor_db.demo").show();
+-----+---+----+
|lh_id| id|name|
+-----+---+----+
+-----+---+----+

from pyspark.sql import functions as F
from pyspark.sql import Window

# Read CSV
df = spark.read.option("header", "true").csv("demo.csv")

# Cast id column to int
df = df.withColumn("id", F.col("id").cast("int"))

# Add auto-increment style column
df = df.withColumn("lh_id", F.row_number().over(Window.orderBy("id")))

# Show before writing
df.show()

# Append to Iceberg table
df.writeTo("vendor_catalog.vendor_db.demo").append()

/home/ubuntu/spark-3.5.6-bin-hadoop3/bin/spark-submit  \ 
--jars /home/ubuntu/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.4_2.12-1.4.2.jar \  
--conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \  
--conf spark.sql.catalog.vendor_catalog=org.apache.iceberg.spark.SparkCatalog \  
--conf spark.sql.catalog.vendor_catalog.type=hadoop \  
--conf spark.sql.catalog.vendor_catalog.warehouse=/tmp/warehouse \  
/home/ubuntu/nifi/scripts/IngestToIceberg.py


wget https://dl.min.io/client/mc/release/linux-amd64/mc -O mc
chmod +x mc
sudo mv mc /usr/local/bin/



spark-sql \
  --conf spark.sql.catalog.rest=org.apache.iceberg.rest.RESTCatalog \
  --conf spark.sql.catalog.rest.uri=http://rest:8181 \
  --conf spark.sql.catalog.rest.io-impl=org.apache.iceberg.aws.s3.S3FileIO \
  --conf spark.sql.catalog.rest.warehouse=s3://warehouse/ \
  --conf spark.sql.catalog.rest.s3.endpoint=http://minio:9000

wget https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-rest-runtime/1.4.2/iceberg-rest-runtime-1.4.2.jar \
     -O ~/.ivy2/jars/org.apache.iceberg_iceberg-rest-runtime-1.4.2.jar


~/spark-3.5.6-bin-hadoop3/bin/pyspark \
--jars ~/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.4_2.12-1.4.2.jar \
--conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
--conf spark.sql.catalog.vendor_catalog=org.apache.iceberg.spark.SparkCatalog \
--conf spark.sql.catalog.vendor_catalog.type=hadoop \
--conf spark.sql.catalog.vendor_catalog.warehouse=s3a://iceberg-warehouse/ \
--conf spark.hadoop.fs.s3a.endpoint=http://localhost:9000 \
--conf spark.hadoop.fs.s3a.access.key=<YOUR_MINIO_ACCESS_KEY> \
--conf spark.hadoop.fs.s3a.secret.key=<YOUR_MINIO_SECRET_KEY> \
--conf spark.hadoop.fs.s3a.path.style.access=true

